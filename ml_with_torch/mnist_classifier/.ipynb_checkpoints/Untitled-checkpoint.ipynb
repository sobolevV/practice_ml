{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_np_img(data):\n",
    "    return data.astype(float) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(file_name, y_column, batch_size=32, valid_size=0.25):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(file_name).astype(float)\n",
    "    df = df.sample(frac=1)  # shuffle dataframe\n",
    "    train_size, valid_size = int(df.shape[0]*(1-valid_size)), int(df.shape[0]*(valid_size))\n",
    "    print(f\"train size = {train_size}, validation size = {valid_size}\")\n",
    "    \n",
    "    y_train_np_data, y_valid_np_data = df[y_column].head(train_size).values, df[y_column].tail(valid_size).values\n",
    "    x_train_np_data, x_valid_np_data = (df.drop(columns=[y_column]).head(train_size).values, \n",
    "                                        df.drop(columns=[y_column]).tail(valid_size).values)\n",
    "    \n",
    "    x_train_np_data, x_valid_np_data = norm_np_img(x_train_np_data), norm_np_img(x_valid_np_data)\n",
    "    print(f\"X train shape = {x_train_np_data.shape}, Y train shape = {y_train_np_data.shape}\")\n",
    "    \n",
    "    y_train_tensor, y_valid_tensor = torch.from_numpy(y_train_np_data), torch.from_numpy(y_valid_np_data)\n",
    "    x_train_tensor, x_valid_tensor = torch.from_numpy(x_train_np_data), torch.from_numpy(x_valid_np_data)\n",
    "    \n",
    "    tensor_ds_train = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)  # create TRAIN tensor dataset\n",
    "    tensor_ds_valid = torch.utils.data.TensorDataset(x_valid_tensor, y_valid_tensor)  # create VALID tensor dataset\n",
    "    \n",
    "    tensor_dl_train = torch.utils.data.DataLoader(tensor_ds_train, batch_size=batch_size, shuffle=True)\n",
    "    tensor_dl_valid = torch.utils.data.DataLoader(tensor_ds_valid, batch_size=batch_size*2)\n",
    "    \n",
    "    return (tensor_dl_train, tensor_dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaLoader:\n",
    "    # Iterable obj\n",
    "    \n",
    "    def __init__(self, DataLoader):\n",
    "        self.dl = DataLoader\n",
    "        self.dev = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "        print('Device is', self.dev)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iter_dl = iter(self.dl)\n",
    "        for xb, yb in iter_dl:\n",
    "            yield self.preprocess(xb, yb)\n",
    "    \n",
    "    def preprocess(self, xb, yb):\n",
    "        return xb.view(-1, 1, 28, 28).to(self.dev), yb.view(-1, 1).to(self.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 31500, validation size = 10500\n",
      "X train shape = (31500, 784), Y train shape = (31500,)\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = load_data_from_csv('mnist_dataset\\\\train.csv', y_column='label', batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0\n",
      "Device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_cdl, valid_cdl = CudaLoader(train_dl), CudaLoader(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(18432, 32)  # bad value - 18432\n",
    "        self.out = nn.Linear(32, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        c2 = F.max_pool2d(F.relu(self.conv2(c1)), (2, 2))\n",
    "        c3 = F.max_pool2d(F.relu(self.conv3(c2)), (2, 2))\n",
    "        \n",
    "        c3 = c3.view(-1, 18432)\n",
    "        f1 = F.relu(self.fc1(c3))\n",
    "        out = self.out(f1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Net(output_size=1).double().cuda()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_func, train_loader, valid_loader=None):\n",
    "    for i in range(epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-292d1373ad9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_cdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-8e41243383e5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, optimizer, loss_func, train_loader, valid_loader)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2215\u001b[1;33m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[0;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "train(model, epochs=1, optimizer=optim, loss_func=loss_func, train_loader=train_cdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_cdl:\n",
    "    print(yb.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env]",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
